{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CASO DE NEGOCIO: Modelamiento de Retención de Clientes en Scholastic Travel Company\n",
    "\n",
    "## Sinopsis\n",
    "\n",
    "Scholastic Travel Company (STC) quiere usar los datos de sus clientes para predecir quién comprará un paquete de viaje el próximo año. Los casos presentan datos de 2.389 clientes, donde el conjunto A contiene varios campos de perfil y el conjunto B contiene información adicional del Net Promoted Score (NPS)\n",
    "\n",
    "Este notebook ha sido preparado con información provista en el Caso de Negocios de Harvard UV7579 (Agosto 23, 2018), Retention Modeling at Scholastic Travel Company, elaborado por Anton Ovchinnikov, profesor de Management Science, Operations Management y Customer Analytics, en la escuela de negocios Smith, de Queen’s University en Canadá.\n",
    "\n",
    "## Objetivos\n",
    "\n",
    "Los objetivos del ejercicio son:\n",
    "\n",
    "1.   Entender el problema de negocio presentado y cómo este problema se convierte en un problema de analítica de datos\n",
    "2.   Aplicar técnicas adecuadas en preparación de datos para clasificación\n",
    "3.   Aplicar técnicas adecuadas en limpieza de datos: lidiar con valores faltantes, categorías extrañas\n",
    "4.   Practicar con herramientas avanzadas de analítica de datos como regresión logística (incluyendo la selección de variables con StepAIC or Recusrsive Feature Elimination para Python) y máquinas de vectores de soporte (SVM)\n",
    "5.   Entender las métricas de casificación más comunes: matriz de confusión, curva ROC, Área bajo la curva AUC, para poder comparar modelos analíticos\n",
    "\n",
    "Los principales pasos a seguir son:\n",
    "\n",
    "1.   Asegurar que los paquetes y librerías necesarios están instalados (e instalarlos si no lo están)\n",
    "2.   Cargar los paquetes y librerías necesarios\n",
    "3.   Cargar los datos\n",
    "4.   \"Limpiar\" los datos: necesitaremos (4.1) convertir algunas características en tipos correctos, (4.2) combinar categorías de tasas, (4.3) arreglar los valores que faltan, y (4.4) crear dummies (one hot encoding) para las características no numéricas\n",
    "5.   Definir el objetivo (la variable que intentamos predecir) y la matriz de características (todas las demás, excepto el ID)\n",
    "6.   Dividir los datos en entrenamiento y prueba\n",
    "7.   Entrenar (ajustar) el modelo con los datos de entrenamiento \n",
    "8.   Aplicarlo a los datos de prueba \n",
    "9.   Calcular las métricas del modelo y ajustar los hiperparámetros para mejorar esas métricas\n",
    "10.  Exportar las predicciones para la toma de decisiones\n",
    "\n",
    "\n",
    "Consideraremos varios modelos de analítica de datos:\n",
    "\n",
    "1. REGRESIÓN LOGÍSTICA: una gneeralización de la regresión -- un modelo que, como la regresión lineal, sigue teniendo coeficientes, valores p y similares, pero aplica transformaciones especiales para tener en cuenta el hecho de que predice categorias en vez de cantidades continuas\n",
    "\n",
    "2. ÁRBOLES DE DECISIÓN - CART: un modelo que es conceptualmente diferente de la regresión, ya que no tiene coeficientes. Veremos que este método es útil para la visualización y la comprensión, pero no es muy preciso para las predicciones\n",
    "\n",
    "3. MÁQUINAS DE VECTORES DE SOPORTE: un método intermedio entre las regresiones y los árboles que se ajusta a planos de menor dimensión para separar los datos en clases con el máximo margen entre ellas\n",
    "\n",
    "4. \n",
    "\n",
    "5.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paso 0: Para iniciar ... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "env: OMP_NUM_THREADS=4\n"
     ]
    }
   ],
   "source": [
    "# Activar múltiples hilos en su computador, para un cálculo más rápido \n",
    "%env OMP_NUM_THREADS = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pasos 1 y 2: Instalar y cargar los paquetes y librerías necesarias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paso 1: Chequear el ambiente Conda y los paquetes/librerías instalados\n",
    "# import sys\n",
    "# !conda env list\n",
    "# !conda list\n",
    "# !conda update --all\n",
    "\n",
    "# Descargar e instalar pandas, numpy, scikit-learn. Podría ser necesario hacerlos desde el prompt de Anaconda\n",
    "# !conda install pandas # pandas includes numpy \n",
    "# !conda install scikit-learn\n",
    "\n",
    "# Paso 2: Cargar los paquetes y librerías necesarios \n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score, classification_report, confusion_matrix, make_scorer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paso 3: Cargar los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   ID Program.Code  From.Grade  To.Grade Group.State  Is.Non.Annual.  Days  \\\n",
       "0   1           HS         4.0       4.0          CA               0     1   \n",
       "1   2           HC         8.0       8.0          AZ               0     7   \n",
       "2   3           HD         8.0       8.0          FL               0     3   \n",
       "3   4           HN         9.0      12.0          VA               1     3   \n",
       "4   5           HD         6.0       8.0          FL               0     6   \n",
       "\n",
       "  Travel.Type  Departure.Date  Return.Date  ...  GroupGradeTypeLow  \\\n",
       "0           A           40557        40557  ...                  K   \n",
       "1           A           40557        40564  ...             Middle   \n",
       "2           A           40558        40560  ...             Middle   \n",
       "3           B           40558        40560  ...          Undefined   \n",
       "4           T           40559        40564  ...             Middle   \n",
       "\n",
       "  GroupGradeTypeHigh        GroupGradeType  MajorProgramCode  \\\n",
       "0         Elementary         K->Elementary                 H   \n",
       "1             Middle        Middle->Middle                 H   \n",
       "2             Middle        Middle->Middle                 H   \n",
       "3          Undefined  Undefined->Undefined                 H   \n",
       "4             Middle        Middle->Middle                 H   \n",
       "\n",
       "   SingleGradeTripFlag  FPP.to.School.enrollment  FPP.to.PAX  \\\n",
       "0                    1                  0.063646    0.936508   \n",
       "1                    1                  0.025882    0.880000   \n",
       "2                    1                  0.025131    0.888889   \n",
       "3                    0                       NaN    1.000000   \n",
       "4                    0                  0.112500    0.910112   \n",
       "\n",
       "   Num.of.Non_FPP.PAX  SchoolSizeIndicator  Retained.in.2012.  \n",
       "0                   4                    L                  1  \n",
       "1                   3                    L                  1  \n",
       "2                   3                    L                  1  \n",
       "3                   0                  NaN                  0  \n",
       "4                   8                  M-L                  0  \n",
       "\n",
       "[5 rows x 56 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ID</th>\n      <th>Program.Code</th>\n      <th>From.Grade</th>\n      <th>To.Grade</th>\n      <th>Group.State</th>\n      <th>Is.Non.Annual.</th>\n      <th>Days</th>\n      <th>Travel.Type</th>\n      <th>Departure.Date</th>\n      <th>Return.Date</th>\n      <th>...</th>\n      <th>GroupGradeTypeLow</th>\n      <th>GroupGradeTypeHigh</th>\n      <th>GroupGradeType</th>\n      <th>MajorProgramCode</th>\n      <th>SingleGradeTripFlag</th>\n      <th>FPP.to.School.enrollment</th>\n      <th>FPP.to.PAX</th>\n      <th>Num.of.Non_FPP.PAX</th>\n      <th>SchoolSizeIndicator</th>\n      <th>Retained.in.2012.</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>HS</td>\n      <td>4.0</td>\n      <td>4.0</td>\n      <td>CA</td>\n      <td>0</td>\n      <td>1</td>\n      <td>A</td>\n      <td>40557</td>\n      <td>40557</td>\n      <td>...</td>\n      <td>K</td>\n      <td>Elementary</td>\n      <td>K-&gt;Elementary</td>\n      <td>H</td>\n      <td>1</td>\n      <td>0.063646</td>\n      <td>0.936508</td>\n      <td>4</td>\n      <td>L</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>HC</td>\n      <td>8.0</td>\n      <td>8.0</td>\n      <td>AZ</td>\n      <td>0</td>\n      <td>7</td>\n      <td>A</td>\n      <td>40557</td>\n      <td>40564</td>\n      <td>...</td>\n      <td>Middle</td>\n      <td>Middle</td>\n      <td>Middle-&gt;Middle</td>\n      <td>H</td>\n      <td>1</td>\n      <td>0.025882</td>\n      <td>0.880000</td>\n      <td>3</td>\n      <td>L</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>HD</td>\n      <td>8.0</td>\n      <td>8.0</td>\n      <td>FL</td>\n      <td>0</td>\n      <td>3</td>\n      <td>A</td>\n      <td>40558</td>\n      <td>40560</td>\n      <td>...</td>\n      <td>Middle</td>\n      <td>Middle</td>\n      <td>Middle-&gt;Middle</td>\n      <td>H</td>\n      <td>1</td>\n      <td>0.025131</td>\n      <td>0.888889</td>\n      <td>3</td>\n      <td>L</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>HN</td>\n      <td>9.0</td>\n      <td>12.0</td>\n      <td>VA</td>\n      <td>1</td>\n      <td>3</td>\n      <td>B</td>\n      <td>40558</td>\n      <td>40560</td>\n      <td>...</td>\n      <td>Undefined</td>\n      <td>Undefined</td>\n      <td>Undefined-&gt;Undefined</td>\n      <td>H</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>1.000000</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>HD</td>\n      <td>6.0</td>\n      <td>8.0</td>\n      <td>FL</td>\n      <td>0</td>\n      <td>6</td>\n      <td>T</td>\n      <td>40559</td>\n      <td>40564</td>\n      <td>...</td>\n      <td>Middle</td>\n      <td>Middle</td>\n      <td>Middle-&gt;Middle</td>\n      <td>H</td>\n      <td>0</td>\n      <td>0.112500</td>\n      <td>0.910112</td>\n      <td>8</td>\n      <td>M-L</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 56 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 33
    }
   ],
   "source": [
    "# Paso 3: Cargar los datos desde el archivo CSV en el dataframe llamado df.\n",
    "\n",
    "df = pd.read_csv(r'/Users/aramirez/Documents/Analitica 2/Caso de Negocio/04 CSV data -- STC(A)_numerical dates.csv', header = 0, delimiter=';', decimal='.', na_values='NaN', keep_default_na=True)  \n",
    "df.head() # show the \"head\" -- first 5 rows of the data; note, these are rows 0...4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paso 4: \"Limpiar\" los datos\n",
    "\n",
    "En este caso, los datos se dejan a propósito ligeramente \"sucios\", es decir, se limpian previamente en cierta medida, pero para efectos de aprendizaje todavía quedan algunos elementos de datos \"sucios\":\n",
    "\n",
    "- Algunos campos de datos (variables, características, columnas) tienen tipos incorrectos, por ejemplo, deberían convertirse de números a categorías.\n",
    "\n",
    "- Algunas variables categóricas tienen demasiados valores (niveles), y algunos de los niveles son demasiado raros: por ejemplo, sólo hay un grupo de Bahamas -- estos datos deberían fusionarse en una categoría más poblada\n",
    "\n",
    "- Faltan algunos datos y hay que sustituirlos o imputarlos\n",
    "\n",
    "- Para concluir la limpieza de los datos, tendremos que crear, por supuesto, variables ficticias (\"one hot encodig\") para las variables categóricas "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 2389 entries, 0 to 2388\nData columns (total 56 columns):\n #   Column                          Non-Null Count  Dtype  \n---  ------                          --------------  -----  \n 0   ID                              2389 non-null   int64  \n 1   Program.Code                    2389 non-null   object \n 2   From.Grade                      2262 non-null   float64\n 3   To.Grade                        2239 non-null   float64\n 4   Group.State                     2389 non-null   object \n 5   Is.Non.Annual.                  2389 non-null   int64  \n 6   Days                            2389 non-null   int64  \n 7   Travel.Type                     2389 non-null   object \n 8   Departure.Date                  2389 non-null   int64  \n 9   Return.Date                     2389 non-null   int64  \n 10  Deposit.Date                    2389 non-null   int64  \n 11  Special.Pay                     470 non-null    object \n 12  Tuition                         2389 non-null   int64  \n 13  FRP.Active                      2389 non-null   int64  \n 14  FRP.Cancelled                   2389 non-null   int64  \n 15  FRP.Take.up.percent.            2389 non-null   float64\n 16  Early.RPL                       1716 non-null   float64\n 17  Latest.RPL                      2370 non-null   float64\n 18  Cancelled.Pax                   2389 non-null   int64  \n 19  Total.Discount.Pax              2389 non-null   int64  \n 20  Initial.System.Date             2381 non-null   float64\n 21  Poverty.Code                    1790 non-null   object \n 22  Region                          2389 non-null   object \n 23  CRM.Segment                     2385 non-null   float64\n 24  School.Type                     2389 non-null   object \n 25  Parent.Meeting.Flag             2389 non-null   int64  \n 26  MDR.Low.Grade                   2321 non-null   float64\n 27  MDR.High.Grade                  2321 non-null   float64\n 28  Total.School.Enrollment         2298 non-null   float64\n 29  Income.Level                    2327 non-null   object \n 30  EZ.Pay.Take.Up.Rate             2389 non-null   float64\n 31  School.Sponsor                  2389 non-null   int64  \n 32  SPR.Product.Type                2389 non-null   object \n 33  SPR.New.Existing                2389 non-null   object \n 34  FPP                             2389 non-null   int64  \n 35  Total.Pax                       2389 non-null   int64  \n 36  SPR.Group.Revenue               2389 non-null   int64  \n 37  NumberOfMeetingswithParents     2389 non-null   int64  \n 38  FirstMeeting                    2052 non-null   float64\n 39  LastMeeting                     2052 non-null   float64\n 40  DifferenceTraveltoFirstMeeting  2052 non-null   float64\n 41  DifferenceTraveltoLastMeeting   2052 non-null   float64\n 42  SchoolGradeTypeLow              2389 non-null   object \n 43  SchoolGradeTypeHigh             2389 non-null   object \n 44  SchoolGradeType                 2389 non-null   object \n 45  DepartureMonth                  2389 non-null   object \n 46  GroupGradeTypeLow               2389 non-null   object \n 47  GroupGradeTypeHigh              2389 non-null   object \n 48  GroupGradeType                  2389 non-null   object \n 49  MajorProgramCode                2389 non-null   object \n 50  SingleGradeTripFlag             2389 non-null   int64  \n 51  FPP.to.School.enrollment        2298 non-null   float64\n 52  FPP.to.PAX                      2389 non-null   float64\n 53  Num.of.Non_FPP.PAX              2389 non-null   int64  \n 54  SchoolSizeIndicator             2298 non-null   object \n 55  Retained.in.2012.               2389 non-null   int64  \ndtypes: float64(17), int64(20), object(19)\nmemory usage: 1.0+ MB\n"
     ]
    }
   ],
   "source": [
    "# Limpieza de datos -- parte 1: convertir tipos de datos incorrectos\n",
    "\n",
    "# Algunos de los tipos de datos que maneja Python:\n",
    "# int -- número entero (e.g., 5)\n",
    "# float -- número fraccionario (e.g., 5.25)\n",
    "# object, str -- text (string). Un texto que contiene varios valores no ordenados (e.g., M/F)\n",
    "\n",
    "df.info() # Para chequear qué tipo de datos tenemos \n",
    "\n",
    "# Otros tipos de datos en el paquete pandas:\n",
    "# category -- categoricos, igual que \"factor\" in R (e.g., red/green/blue, or M/F: una lista con varios valores no ordenados)\n",
    "# datetime -- fecha y hora (e.g., 01.01.2020)\n",
    "# bool -- binario (e.g.? yes/no, 1/0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 2389 entries, 0 to 2388\nData columns (total 56 columns):\n #   Column                          Non-Null Count  Dtype   \n---  ------                          --------------  -----   \n 0   ID                              2389 non-null   int64   \n 1   Program.Code                    2389 non-null   object  \n 2   From.Grade                      2262 non-null   category\n 3   To.Grade                        2239 non-null   category\n 4   Group.State                     2389 non-null   category\n 5   Is.Non.Annual.                  2389 non-null   category\n 6   Days                            2389 non-null   int64   \n 7   Travel.Type                     2389 non-null   category\n 8   Departure.Date                  2389 non-null   int64   \n 9   Return.Date                     2389 non-null   int64   \n 10  Deposit.Date                    2389 non-null   int64   \n 11  Special.Pay                     470 non-null    category\n 12  Tuition                         2389 non-null   int64   \n 13  FRP.Active                      2389 non-null   int64   \n 14  FRP.Cancelled                   2389 non-null   int64   \n 15  FRP.Take.up.percent.            2389 non-null   float64 \n 16  Early.RPL                       1716 non-null   float64 \n 17  Latest.RPL                      2370 non-null   float64 \n 18  Cancelled.Pax                   2389 non-null   int64   \n 19  Total.Discount.Pax              2389 non-null   int64   \n 20  Initial.System.Date             2381 non-null   float64 \n 21  Poverty.Code                    1790 non-null   category\n 22  Region                          2389 non-null   category\n 23  CRM.Segment                     2385 non-null   category\n 24  School.Type                     2389 non-null   category\n 25  Parent.Meeting.Flag             2389 non-null   category\n 26  MDR.Low.Grade                   2321 non-null   category\n 27  MDR.High.Grade                  2321 non-null   category\n 28  Total.School.Enrollment         2298 non-null   float64 \n 29  Income.Level                    2327 non-null   category\n 30  EZ.Pay.Take.Up.Rate             2389 non-null   float64 \n 31  School.Sponsor                  2389 non-null   category\n 32  SPR.Product.Type                2389 non-null   category\n 33  SPR.New.Existing                2389 non-null   category\n 34  FPP                             2389 non-null   int64   \n 35  Total.Pax                       2389 non-null   int64   \n 36  SPR.Group.Revenue               2389 non-null   int64   \n 37  NumberOfMeetingswithParents     2389 non-null   int64   \n 38  FirstMeeting                    2052 non-null   float64 \n 39  LastMeeting                     2052 non-null   float64 \n 40  DifferenceTraveltoFirstMeeting  2052 non-null   float64 \n 41  DifferenceTraveltoLastMeeting   2052 non-null   float64 \n 42  SchoolGradeTypeLow              2389 non-null   category\n 43  SchoolGradeTypeHigh             2389 non-null   category\n 44  SchoolGradeType                 2389 non-null   category\n 45  DepartureMonth                  2389 non-null   object  \n 46  GroupGradeTypeLow               2389 non-null   category\n 47  GroupGradeTypeHigh              2389 non-null   category\n 48  GroupGradeType                  2389 non-null   object  \n 49  MajorProgramCode                2389 non-null   category\n 50  SingleGradeTripFlag             2389 non-null   category\n 51  FPP.to.School.enrollment        2298 non-null   float64 \n 52  FPP.to.PAX                      2389 non-null   float64 \n 53  Num.of.Non_FPP.PAX              2389 non-null   int64   \n 54  SchoolSizeIndicator             2298 non-null   category\n 55  Retained.in.2012.               2389 non-null   category\ndtypes: category(26), float64(12), int64(15), object(3)\nmemory usage: 629.3+ KB\n"
     ]
    }
   ],
   "source": [
    "# Limpieza de los datos -- parte 1: conversión de tipos de datos que deberían ser categóricos\n",
    "\n",
    "df['From.Grade'] = df['From.Grade'].astype('category')\n",
    "df['To.Grade'] = df['To.Grade'].astype('category')\n",
    "df['Group.State'] = df['Group.State'].astype('category')\n",
    "df['Is.Non.Annual.'] = df['Is.Non.Annual.'].astype('category')\n",
    "df['Travel.Type'] = df['Travel.Type'].astype('category')\n",
    "df['Poverty.Code'] = df['Poverty.Code'].astype('category')\n",
    "df['CRM.Segment'] = df['CRM.Segment'].astype('category')\n",
    "df['School.Type'] = df['School.Type'].astype('category')\n",
    "df['Parent.Meeting.Flag'] = df['Parent.Meeting.Flag'].astype('category')\n",
    "df['MDR.Low.Grade'] = df['MDR.Low.Grade'].astype('category')\n",
    "df['MDR.High.Grade'] = df['MDR.High.Grade'].astype('category')\n",
    "df['School.Sponsor'] = df['School.Sponsor'].astype('category')\n",
    "df['SchoolGradeTypeLow'] = df['SchoolGradeTypeLow'].astype('category')\n",
    "df['SchoolGradeTypeHigh'] = df['SchoolGradeTypeHigh'].astype('category')\n",
    "df['GroupGradeTypeLow'] = df['GroupGradeTypeLow'].astype('category')\n",
    "df['GroupGradeTypeHigh'] = df['GroupGradeTypeHigh'].astype('category')\n",
    "df['MajorProgramCode'] = df['MajorProgramCode'].astype('category')\n",
    "df['SingleGradeTripFlag'] = df['SingleGradeTripFlag'].astype('category')\n",
    "df['SchoolSizeIndicator'] = df['SchoolSizeIndicator'].astype('category')\n",
    "df['Retained.in.2012.'] = df['Retained.in.2012.'].astype('category')\n",
    "\n",
    "df['Region'] = df['Region'].astype('category')\n",
    "df['Income.Level'] = df['Income.Level'].astype('category')\n",
    "df['Special.Pay'] = df['Special.Pay'].astype('category')\n",
    "df['SPR.Product.Type'] = df['SPR.Product.Type'].astype('category')\n",
    "df['SchoolGradeType'] = df['SchoolGradeType'].astype('category')\n",
    "df['SPR.New.Existing'] = df['SPR.New.Existing'].astype('category')\n",
    "\n",
    "\n",
    "df.info() # Chequeemos los resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.08           9\n",
       "0.04           9\n",
       "0.1            9\n",
       "0.066666667    8\n",
       "0.05           7\n",
       "              ..\n",
       "0.011494253    1\n",
       "0.032547699    1\n",
       "0.018469657    1\n",
       "0.088628763    1\n",
       "0.080838323    1\n",
       "Name: FPP.to.School.enrollment, Length: 1909, dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 30
    }
   ],
   "source": [
    "df['FPP.to.School.enrollment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "CA                718\n",
       "TX                308\n",
       "WA                147\n",
       "IL                104\n",
       "CO                 89\n",
       "MI                 71\n",
       "FL                 62\n",
       "OH                 53\n",
       "AZ                 53\n",
       "OR                 51\n",
       "MN                 51\n",
       "WI                 46\n",
       "IN                 43\n",
       "MO                 43\n",
       "NE                 42\n",
       "TN                 38\n",
       "MA                 36\n",
       "IA                 35\n",
       "OK                 33\n",
       "LA                 31\n",
       "KS                 26\n",
       "GA                 22\n",
       "AL                 21\n",
       "NV                 20\n",
       "NM                 20\n",
       "NY                 19\n",
       "VA                 18\n",
       "KY                 16\n",
       "NC                 16\n",
       "MD                 15\n",
       "CT                 15\n",
       "ID                 14\n",
       "SD                 11\n",
       "SC                 10\n",
       "AR                 10\n",
       "MS                  9\n",
       "UT                  9\n",
       "HI                  9\n",
       "NH                  7\n",
       "ME                  7\n",
       "NJ                  6\n",
       "MT                  6\n",
       "ND                  5\n",
       "PA                  5\n",
       "AK                  5\n",
       "MX                  3\n",
       "RI                  3\n",
       "WY                  2\n",
       "PR                  1\n",
       "VT                  1\n",
       "Cayman Islands      1\n",
       "Bermuda             1\n",
       "WV                  1\n",
       "AB                  1\n",
       "Name: Group.State, dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 37
    }
   ],
   "source": [
    "# Limpieza de datos -- parte 2: combinar categorías poco frecuentes (\"niveles\")\n",
    "\n",
    "df['Group.State'].value_counts() # Tomemos el ejemplo de la variable Group.State\n",
    "\n",
    "# El siguiente es el código para chequear todas las columnas: \n",
    "# for col in df.select_dtypes(include=['category','object','bool']).columns:\n",
    "#    print(col)\n",
    "#    print(df[col].value_counts())\n",
    "#    print('\\n') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   ID        Program.Code From.Grade To.Grade        Group.State  \\\n",
       "0   1                  HS        4.0      4.0                 CA   \n",
       "1   2                  HC        8.0      8.0                 AZ   \n",
       "2   3                  HD        8.0      8.0                 FL   \n",
       "3   4                  HN        9.0     12.0                 VA   \n",
       "4   5                  HD        6.0      8.0                 FL   \n",
       "5   6                  HC       10.0     12.0                 LA   \n",
       "6   7                  SG       11.0     12.0                 MA   \n",
       "7   8  Other_Program.Code        9.0      9.0  Other_Group.State   \n",
       "8   9                  CC        8.0      8.0                 AZ   \n",
       "9  10                  HD        8.0      8.0                 TX   \n",
       "\n",
       "  Is.Non.Annual.  Days        Travel.Type  Departure.Date  Return.Date  ...  \\\n",
       "0              0     1                  A           40557        40557  ...   \n",
       "1              0     7                  A           40557        40564  ...   \n",
       "2              0     3                  A           40558        40560  ...   \n",
       "3              1     3                  B           40558        40560  ...   \n",
       "4              0     6  Other_Travel.Type           40559        40564  ...   \n",
       "5              0     4                  A           40560        40563  ...   \n",
       "6              1     6                  A           40561        40566  ...   \n",
       "7              0     8                  A           40567        40574  ...   \n",
       "8              0     8                  A           40572        40579  ...   \n",
       "9              0     4                  A           40581        40584  ...   \n",
       "\n",
       "   GroupGradeTypeLow GroupGradeTypeHigh        GroupGradeType  \\\n",
       "0                  K         Elementary         K->Elementary   \n",
       "1             Middle             Middle        Middle->Middle   \n",
       "2             Middle             Middle        Middle->Middle   \n",
       "3          Undefined          Undefined  Undefined->Undefined   \n",
       "4             Middle             Middle        Middle->Middle   \n",
       "5               High               High            High->High   \n",
       "6               High               High            High->High   \n",
       "7          Undefined          Undefined  Undefined->Undefined   \n",
       "8             Middle               High          Middle->High   \n",
       "9                 PK             Middle            PK->Middle   \n",
       "\n",
       "   MajorProgramCode  SingleGradeTripFlag  FPP.to.School.enrollment  \\\n",
       "0                 H                    1                  0.063646   \n",
       "1                 H                    1                  0.025882   \n",
       "2                 H                    1                  0.025131   \n",
       "3                 H                    0                       NaN   \n",
       "4                 H                    0                  0.112500   \n",
       "5                 H                    0                  0.010650   \n",
       "6                 S                    0                  0.111111   \n",
       "7                 I                    1                       NaN   \n",
       "8                 C                    1                  0.104000   \n",
       "9                 H                    1                  0.103937   \n",
       "\n",
       "   FPP.to.PAX  Num.of.Non_FPP.PAX  SchoolSizeIndicator  Retained.in.2012.  \n",
       "0    0.936508                   4                    L                  1  \n",
       "1    0.880000                   3                    L                  1  \n",
       "2    0.888889                   3                    L                  1  \n",
       "3    1.000000                   0                  NaN                  0  \n",
       "4    0.910112                   8                  M-L                  0  \n",
       "5    0.909091                   1                    L                  1  \n",
       "6    0.925926                   2                    S                  0  \n",
       "7    0.928571                   1                  NaN                  0  \n",
       "8    0.928571                   4                  S-M                  1  \n",
       "9    0.916667                   6                  M-L                  1  \n",
       "\n",
       "[10 rows x 56 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ID</th>\n      <th>Program.Code</th>\n      <th>From.Grade</th>\n      <th>To.Grade</th>\n      <th>Group.State</th>\n      <th>Is.Non.Annual.</th>\n      <th>Days</th>\n      <th>Travel.Type</th>\n      <th>Departure.Date</th>\n      <th>Return.Date</th>\n      <th>...</th>\n      <th>GroupGradeTypeLow</th>\n      <th>GroupGradeTypeHigh</th>\n      <th>GroupGradeType</th>\n      <th>MajorProgramCode</th>\n      <th>SingleGradeTripFlag</th>\n      <th>FPP.to.School.enrollment</th>\n      <th>FPP.to.PAX</th>\n      <th>Num.of.Non_FPP.PAX</th>\n      <th>SchoolSizeIndicator</th>\n      <th>Retained.in.2012.</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>HS</td>\n      <td>4.0</td>\n      <td>4.0</td>\n      <td>CA</td>\n      <td>0</td>\n      <td>1</td>\n      <td>A</td>\n      <td>40557</td>\n      <td>40557</td>\n      <td>...</td>\n      <td>K</td>\n      <td>Elementary</td>\n      <td>K-&gt;Elementary</td>\n      <td>H</td>\n      <td>1</td>\n      <td>0.063646</td>\n      <td>0.936508</td>\n      <td>4</td>\n      <td>L</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>HC</td>\n      <td>8.0</td>\n      <td>8.0</td>\n      <td>AZ</td>\n      <td>0</td>\n      <td>7</td>\n      <td>A</td>\n      <td>40557</td>\n      <td>40564</td>\n      <td>...</td>\n      <td>Middle</td>\n      <td>Middle</td>\n      <td>Middle-&gt;Middle</td>\n      <td>H</td>\n      <td>1</td>\n      <td>0.025882</td>\n      <td>0.880000</td>\n      <td>3</td>\n      <td>L</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>HD</td>\n      <td>8.0</td>\n      <td>8.0</td>\n      <td>FL</td>\n      <td>0</td>\n      <td>3</td>\n      <td>A</td>\n      <td>40558</td>\n      <td>40560</td>\n      <td>...</td>\n      <td>Middle</td>\n      <td>Middle</td>\n      <td>Middle-&gt;Middle</td>\n      <td>H</td>\n      <td>1</td>\n      <td>0.025131</td>\n      <td>0.888889</td>\n      <td>3</td>\n      <td>L</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>HN</td>\n      <td>9.0</td>\n      <td>12.0</td>\n      <td>VA</td>\n      <td>1</td>\n      <td>3</td>\n      <td>B</td>\n      <td>40558</td>\n      <td>40560</td>\n      <td>...</td>\n      <td>Undefined</td>\n      <td>Undefined</td>\n      <td>Undefined-&gt;Undefined</td>\n      <td>H</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>1.000000</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>HD</td>\n      <td>6.0</td>\n      <td>8.0</td>\n      <td>FL</td>\n      <td>0</td>\n      <td>6</td>\n      <td>Other_Travel.Type</td>\n      <td>40559</td>\n      <td>40564</td>\n      <td>...</td>\n      <td>Middle</td>\n      <td>Middle</td>\n      <td>Middle-&gt;Middle</td>\n      <td>H</td>\n      <td>0</td>\n      <td>0.112500</td>\n      <td>0.910112</td>\n      <td>8</td>\n      <td>M-L</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>6</td>\n      <td>HC</td>\n      <td>10.0</td>\n      <td>12.0</td>\n      <td>LA</td>\n      <td>0</td>\n      <td>4</td>\n      <td>A</td>\n      <td>40560</td>\n      <td>40563</td>\n      <td>...</td>\n      <td>High</td>\n      <td>High</td>\n      <td>High-&gt;High</td>\n      <td>H</td>\n      <td>0</td>\n      <td>0.010650</td>\n      <td>0.909091</td>\n      <td>1</td>\n      <td>L</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>7</td>\n      <td>SG</td>\n      <td>11.0</td>\n      <td>12.0</td>\n      <td>MA</td>\n      <td>1</td>\n      <td>6</td>\n      <td>A</td>\n      <td>40561</td>\n      <td>40566</td>\n      <td>...</td>\n      <td>High</td>\n      <td>High</td>\n      <td>High-&gt;High</td>\n      <td>S</td>\n      <td>0</td>\n      <td>0.111111</td>\n      <td>0.925926</td>\n      <td>2</td>\n      <td>S</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>8</td>\n      <td>Other_Program.Code</td>\n      <td>9.0</td>\n      <td>9.0</td>\n      <td>Other_Group.State</td>\n      <td>0</td>\n      <td>8</td>\n      <td>A</td>\n      <td>40567</td>\n      <td>40574</td>\n      <td>...</td>\n      <td>Undefined</td>\n      <td>Undefined</td>\n      <td>Undefined-&gt;Undefined</td>\n      <td>I</td>\n      <td>1</td>\n      <td>NaN</td>\n      <td>0.928571</td>\n      <td>1</td>\n      <td>NaN</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>9</td>\n      <td>CC</td>\n      <td>8.0</td>\n      <td>8.0</td>\n      <td>AZ</td>\n      <td>0</td>\n      <td>8</td>\n      <td>A</td>\n      <td>40572</td>\n      <td>40579</td>\n      <td>...</td>\n      <td>Middle</td>\n      <td>High</td>\n      <td>Middle-&gt;High</td>\n      <td>C</td>\n      <td>1</td>\n      <td>0.104000</td>\n      <td>0.928571</td>\n      <td>4</td>\n      <td>S-M</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>10</td>\n      <td>HD</td>\n      <td>8.0</td>\n      <td>8.0</td>\n      <td>TX</td>\n      <td>0</td>\n      <td>4</td>\n      <td>A</td>\n      <td>40581</td>\n      <td>40584</td>\n      <td>...</td>\n      <td>PK</td>\n      <td>Middle</td>\n      <td>PK-&gt;Middle</td>\n      <td>H</td>\n      <td>1</td>\n      <td>0.103937</td>\n      <td>0.916667</td>\n      <td>6</td>\n      <td>M-L</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>10 rows × 56 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 38
    }
   ],
   "source": [
    "\n",
    "# Presentamos una función personalizada que llamaremos CombineRareCategories\n",
    "# esta función tiene dos argumentos: el nombre del dataframe (data) y el número mínimo de puntos de datos para seguir \n",
    "# siendo una categoría separada (mincount)\n",
    "# esta función recorrerá todas las columnas del marco de datos y combinará todas las categorías que aparezcan en los \n",
    "# datos menos que el número mínimo de veces en (Other)\n",
    "\n",
    "def CombineRareCategories(data, mincount):\n",
    "    for col in data.columns:\n",
    "        if (type(data[col][0]) == str):\n",
    "            for index, row in pd.DataFrame(data[col].value_counts()).iterrows():\n",
    "                if ( row[0] < mincount):\n",
    "                    df[col].replace(index, 'Other_' + col, inplace = True)\n",
    "                else:\n",
    "                    None\n",
    "\n",
    "# Aplicamos esta función a variables con mincount=10                    \n",
    "CombineRareCategories(df, 10)        \n",
    "\n",
    "df[0:10] # Chequeamos los resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "ID                                   0\n",
       "Program.Code                         0\n",
       "From.Grade                         127\n",
       "To.Grade                           150\n",
       "Group.State                          0\n",
       "Is.Non.Annual.                       0\n",
       "Days                                 0\n",
       "Travel.Type                          0\n",
       "Departure.Date                       0\n",
       "Return.Date                          0\n",
       "Deposit.Date                         0\n",
       "Special.Pay                       1919\n",
       "Tuition                              0\n",
       "FRP.Active                           0\n",
       "FRP.Cancelled                        0\n",
       "FRP.Take.up.percent.                 0\n",
       "Early.RPL                          673\n",
       "Latest.RPL                          19\n",
       "Cancelled.Pax                        0\n",
       "Total.Discount.Pax                   0\n",
       "Initial.System.Date                  8\n",
       "Poverty.Code                       599\n",
       "Region                               0\n",
       "CRM.Segment                          4\n",
       "School.Type                          0\n",
       "Parent.Meeting.Flag                  0\n",
       "MDR.Low.Grade                       68\n",
       "MDR.High.Grade                      68\n",
       "Total.School.Enrollment             91\n",
       "Income.Level                        62\n",
       "EZ.Pay.Take.Up.Rate                  0\n",
       "School.Sponsor                       0\n",
       "SPR.Product.Type                     0\n",
       "SPR.New.Existing                     0\n",
       "FPP                                  0\n",
       "Total.Pax                            0\n",
       "SPR.Group.Revenue                    0\n",
       "NumberOfMeetingswithParents          0\n",
       "FirstMeeting                       337\n",
       "LastMeeting                        337\n",
       "DifferenceTraveltoFirstMeeting     337\n",
       "DifferenceTraveltoLastMeeting      337\n",
       "SchoolGradeTypeLow                   0\n",
       "SchoolGradeTypeHigh                  0\n",
       "SchoolGradeType                      0\n",
       "DepartureMonth                       0\n",
       "GroupGradeTypeLow                    0\n",
       "GroupGradeTypeHigh                   0\n",
       "GroupGradeType                       0\n",
       "MajorProgramCode                     0\n",
       "SingleGradeTripFlag                  0\n",
       "FPP.to.School.enrollment            91\n",
       "FPP.to.PAX                           0\n",
       "Num.of.Non_FPP.PAX                   0\n",
       "SchoolSizeIndicator                 91\n",
       "Retained.in.2012.                    0\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 39
    }
   ],
   "source": [
    "# Limpieza de datos -- parte 3: Reemplazo/Imputación de datos faltantes\n",
    "\n",
    "pd.DataFrame(df).isna().sum() # Chequeamos si hay datos faltantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "       Poverty.Code  Poverty.Code_surrogate\n",
       "0                 B                     0.0\n",
       "1                 C                     0.0\n",
       "2                 C                     0.0\n",
       "3     missing_value                     1.0\n",
       "4                 D                     0.0\n",
       "...             ...                     ...\n",
       "2384              C                     0.0\n",
       "2385              C                     0.0\n",
       "2386  missing_value                     1.0\n",
       "2387  missing_value                     1.0\n",
       "2388              B                     0.0\n",
       "\n",
       "[2389 rows x 2 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Poverty.Code</th>\n      <th>Poverty.Code_surrogate</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>B</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>C</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>C</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>missing_value</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>D</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2384</th>\n      <td>C</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2385</th>\n      <td>C</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2386</th>\n      <td>missing_value</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>2387</th>\n      <td>missing_value</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>2388</th>\n      <td>B</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>2389 rows × 2 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 40
    }
   ],
   "source": [
    "# Método:\n",
    "# Variables Categóricas: agregar una nueva categoría 'missing_value' (como si fuera un nuevo color, o género)\n",
    "# Variables Numéricas: reemplazar con la mediana (o la media, o el valor más frecuente, etc.) Un método alterno es\n",
    "# ejecutar una imputación, see here: https://scikit-learn.org/stable/modules/impute.html \n",
    "# + agregamos columnas sustitutas indicando que el valor ha sido imputado\n",
    "\n",
    "# creación de variables sustitutas\n",
    "for col in df:\n",
    "    if df[col].isna().sum() != 0: \n",
    "        df[col + '_surrogate'] = df[col].isna().astype(int)\n",
    "\n",
    "# fijación de variables categóricas\n",
    "imputer = SimpleImputer(missing_values = np.nan, strategy='constant')\n",
    "imputer.fit(df.select_dtypes(exclude=['int64','float64']))\n",
    "df[df.select_dtypes(exclude=['int64','float64']).columns] = imputer.transform(df.select_dtypes(exclude=['int64','float64']))\n",
    "           \n",
    "# fijación de variables numéricas \n",
    "imputer = SimpleImputer(missing_values = np.nan, strategy='median')\n",
    "imputer.fit(df.select_dtypes(include=['int64','float64']))\n",
    "df[df.select_dtypes(include=['int64','float64']).columns] = imputer.transform(df.select_dtypes(include=['int64','float64']))\n",
    "\n",
    "# Examinemos los resultados para la variable \"Poverty.Code\"\n",
    "df[['Poverty.Code','Poverty.Code_surrogate']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limpieza de datos -- parte 4: creación de dummies para variables no numéricas (\"one hot encoding\")\n",
    "\n",
    "df = pd.get_dummies(df, columns = df.select_dtypes(exclude=['int64','float64']).columns, drop_first = True)\n",
    "\n",
    "pd.options.display.max_columns = None # remove the limit on the number of columns by default only 20 are shows\n",
    "\n",
    "df.head()  # nuestro dataset tiene ahora 212 columnas (!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paso 5:  Definición del vector objetivo (y) y la matriz de características (X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['Retained.in.2012.']\n",
    "X = df.drop(columns = 'Retained.in.2012.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paso 6:  Dividir X, y en entrenamiento y prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos la semilla para el generador de número aleatorios\n",
    "np.random.seed(77300)\n",
    "\n",
    "# Dividimos los datos aleatoriamente en 80% para entrenamiento y 20% para prueba \n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.20, stratify=y)\n",
    "# IMPORTANTE: Las muestras están estratificadas, i.e., la proporción de clientes retenidos y no-retenidos es la misma en ambos\n",
    "\n",
    "# Chequeemos los resultados\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.countplot(x=y_test, palette=\"bright\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pasos 7, 8, 9: Desarrollar un modelo con los datos de entrenamiento, Usarlo para predecir los valores en los datos de prueba, Calcular las métricas del modelo, y comparar modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Primero, definimos un conjunto de funciones para calcular las métricas del modelo\n",
    "\n",
    "# Curva ROC\n",
    "def plot_roc(y_test, y_pred):\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, y_pred, pos_label=1, drop_intermediate = False)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    plt.figure()\n",
    "    lw = 2\n",
    "    plt.plot(fpr, tpr, color='darkorange',\n",
    "             lw=lw, label='ROC curve (AUC = %0.2f)' % roc_auc)\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "    plt.xlim([-0.001, 1.001])\n",
    "    plt.ylim([-0.001, 1.001])\n",
    "    plt.xlabel('1-Specificity (False Negative Rate)')\n",
    "    plt.ylabel('Sensitivity (True Positive Rate)')\n",
    "    plt.title('ROC curve')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "\n",
    "# Matriz de Confusión: cm[0,0], cm[0,1], cm[1,0], cm[1,1]: tn, fp, fn, tp\n",
    "\n",
    "# Sensitivity\n",
    "def custom_sensitivity_score(y_test, y_pred):\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    tn, fp, fn, tp = cm[0][0], cm[0][1], cm[1][0], cm[1][1]\n",
    "    return (tp/(tp+fn))\n",
    "\n",
    "# Specificity\n",
    "def custom_specificity_score(y_test, y_pred):\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    tn, fp, fn, tp = cm[0][0], cm[0][1], cm[1][0], cm[1][1]\n",
    "    return (tn/(tn+fp))\n",
    "\n",
    "# Positive Predictive Value\n",
    "def custom_ppv_score(y_test, y_pred):\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    tn, fp, fn, tp = cm[0][0], cm[0][1], cm[1][0], cm[1][1]\n",
    "    return (tp/(tp+fp))\n",
    "\n",
    "# Negative Predictive Value\n",
    "def custom_npv_score(y_test, y_pred):\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    tn, fp, fn, tp = cm[0][0], cm[0][1], cm[1][0], cm[1][1]\n",
    "    return (tn/(tn+fn))\n",
    "\n",
    "# Accuracy\n",
    "def custom_accuracy_score(y_test, y_pred):\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    tn, fp, fn, tp = cm[0][0], cm[0][1], cm[1][0], cm[1][1]\n",
    "    return ((tn+tp)/(tn+tp+fn+fp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelo № 1: Regresión Logística\n",
    "\n",
    "Podríamos estar tentados a incluir todas las variables, pero esto haría que el modelo resultante fuera demasiado largo para la selección de variables que sigue. Lo recomendable es realizar un análisis exploratorio de datos donde se apliquen técnicas estadísticas para detectar cuáles variables estarían más relacionadas con la variable dependiente. Por ahora, iniciaremos incluyendo todas las variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos el modelo y lo llamamos classifier_LR\n",
    "classifier_LR = LogisticRegression(solver='liblinear')\n",
    "\n",
    "# Entrenamos classifier_LR con los datos de entrenamiento\n",
    "classifier_LR.fit(X_train, y_train)\n",
    "\n",
    "# Esto es una regresión, por tanto tiene coeficientes -- Revisemoslos\n",
    "# Observe que no hay una manera sencilla de mostrar signficancia, etc. con  sklearn\n",
    "print('Intercepto: ' + str(classifier_LR.intercept_))\n",
    "print('Coeficientes (10 mayores and 10 menores) [recuerde, existen 211, en total: ]')\n",
    "summary = pd.DataFrame([X_test.columns,classifier_LR.coef_[0]]).transpose().sort_values(by = 1, ascending = False)\n",
    "summary.columns = ['Variable','Coeficiente']\n",
    "top10positive = summary.head(10) # 10 más grandes (por valor)\n",
    "top10negative = summary.tail(10) # 10 más pequeños (por valor)\n",
    "top10list=pd.DataFrame()\n",
    "top10list= top10list.append(pd.DataFrame(data = top10positive))\n",
    "top10list= top10list.append(pd.DataFrame(data = top10negative))\n",
    "top10list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usemos el modelo entrenado para predecir sobre los datos de prueba\n",
    "\n",
    "y_pred_prob = classifier_LR.predict_proba(X_test)[:,1] # probabilities\n",
    "\n",
    "# Seleccionamos el valor del umbral -- usaremos Т=0.6073. Porqué? Porqué no 50%? \n",
    "# 60.73% -- es la probabilidad promedio de retención en nuestros datos (1451 \"1\"s de 2389 datos) \n",
    "class_threshold = 0.6073\n",
    "\n",
    "y_pred = np.where(y_pred_prob > class_threshold, 1, 0) # aplicación de la regla del umbral para clasificar\n",
    "\n",
    "print(y_pred_prob[0:5]) # Primeras 5 probabilidades \n",
    "print(y_pred[0:5]) # Resultados pronosticados \n",
    "print(y_test[0:5]) # Resultados reales\n",
    "\n",
    "# oops ... para los primeros 5 clientes nuestro modelo tuvo 2 errores: \n",
    "# en el primer cliente (\"falso positivo\") y en el tercero (\"falso negativo\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Revisemos las métricas del modelo \n",
    "print('Métricas del modelo de regresión logística: \\n')\n",
    "\n",
    "cm = np.transpose(confusion_matrix(y_test, y_pred))\n",
    "print(\"Matriz de confusión: \\n\" + str(cm))\n",
    "\n",
    "print(\"                                   Accuracy: \" + str(custom_accuracy_score(y_test, y_pred))) \n",
    "print(\"                       SENSITIVITY (RECALL): \" + str(custom_sensitivity_score(y_test, y_pred)))\n",
    "print(\"                     SPECIFICITY (FALL-OUT): \" + str(custom_specificity_score(y_test, y_pred)))\n",
    "print(\"     POSITIVE PREDICTIVE VALUE, (PRECISION): \" + str(custom_ppv_score(y_test, y_pred)))\n",
    "print(\"                  NEGATIVE PREDICTIVE VALUE: \" + str(custom_npv_score(y_test, y_pred)))\n",
    "\n",
    "plot_roc(y_test, y_pred_prob)\n",
    "print(\" AUC: \" + str(roc_auc_score(y_test, y_pred_prob)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpretación de la curva ROC y el AUC: \n",
    "\n",
    "- Se sabe que el modelo está cometiendo alrededor del 34% de error, pero este error es uniforme para todos los clientes? Sería algo no deseable puesto que el moodelo no será tan decisivo. Si tomamos un cliente con una probabilidad pronósticada de 99%, uno con una probabilidad de 50%, y uno con una probabilidad del 1%. Esperaríamos que el modelo nos diera una certeza de que el primer cliente será retenido, y una certeza de que el tercero no lo será, pero podríamos estar cómodos con un error grande para el segundo.\n",
    "\n",
    "- La curva ROC muestra justamente eso. Ranquea todos los clientes desde el que tiene la probabilidad más alta de retención hasta la más baja (equivale a variar el umbral desde alto hasta bajo). Comenzando en el origen, mapea todos los clientes en orden descendiente de probabilidad (desde el “mejor” hasta el “peor”). Un clasificador perfecto, con exactitud 100%, primero predeciría correctamente todos los positivos, y luego predeciría correctamente todos los negativos; es decir, la curva iría recto hasta el punto (0,1), y luego cambiaría y sería horizontal hasta el punto (1,1). Esto, por supuesto, no es posible en la práctica, y los “pasos” en la curva reflejan los errores ocasionales que el modelo comete. Un buen modelo cometería pocos errores positivos para los mejores clientes y pocos eroreres negativos para los peores.\n",
    "\n",
    "-  Es importante observar que un modelo que simplemente adivina al azar, tendrá como curva ROC una línea de 45 grados. Tal modelo tendría la misma probabilidad de hacer una predicción correcta que una incorrecta, sin importar si el cliente tiene una alta o baja probabilidad predecida.\n",
    "\n",
    "- En este caso el AUC es 83.12% el cual no es un mal resultado. El AUC indica la proporción de parejas concordantes en los datos; en este caso el porcentaje de parejas concordantes es aproximadamente 83%, lo cual es bueno. Las parejas concordantes son aquellas parejas de casos positivo y negativo en el dataset para las cuales el modelo SVM - con ciertos parámetros - puede clasificarlos correctamente.\n",
    "\n",
    "- En el dataset de prueba, tenemos 290 positivos (clientes retenidos) y 188 negativos (clientes no retenidos); el número total de parejas (positivos y negativos) es 290 x 188 = 54520, de los cuales 73.16% (= 39887) tienen unos parámetros del SVM que pueden clasificarlos correctamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MÉTODO RFE\n",
    "\n",
    "# Aplicamos selección de variables con Stepwise Recursive Feature Selection\n",
    "# El método Recursive Feature Elimination (RFE) es un método para selección de variables.  Funciona removiendo recursivamente\n",
    "# atributos y construye un modelo sobre los atributos restantes.  Usa el accuracy del modelo para identificar cuáles atributos\n",
    "# (y combinación de atributos) contribuyen más significativamente a la predicción del atributo objetivo.\n",
    "\n",
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "rfe = RFE(estimator=classifier_LR, n_features_to_select=20, step=1) # in this example we will select 20 variables; this number \"20\" is a hyperparameter to tune\n",
    "rfe.fit(X_train, y_train)\n",
    "ranking = rfe.ranking_.reshape(len(X_train.columns))\n",
    "\n",
    "# Cuáles son las 20 variables que quedan en el modelo?\n",
    "pd.DataFrame([X_test.columns,ranking]).transpose().sort_values(1).head(30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(ranking)\n",
    "print(rfe.get_support())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenemos el nuevo modelo y llamemoslo classifier_LR_RFE \n",
    "classifier_LR_RFE = rfe.fit(X_train, y_train)\n",
    "\n",
    "# Usemos el modelo entrenado para predecir sobre los datos de prueba\n",
    "y_pred_prob = classifier_LR_RFE.predict_proba(X_test)[:,1] # probabilidades\n",
    "y_pred = np.where(y_pred_prob > class_threshold, 1, 0) # clasificación\n",
    "\n",
    "\n",
    "# Revisemos las métricas del modelo después de la selección de variables \n",
    "print('Métricas del modelo de regresión logística después de la selección de variables: \\n')\n",
    "\n",
    "cm = np.transpose(confusion_matrix(y_test, y_pred))\n",
    "print(\"Matriz de confusión: \\n\" + str(cm))\n",
    "\n",
    "print(\"                                   Accuracy: \" + str(custom_accuracy_score(y_test, y_pred))) \n",
    "print(\"                       SENSITIVITY (RECALL): \" + str(custom_sensitivity_score(y_test, y_pred)))\n",
    "print(\"                     SPECIFICITY (FALL-OUT): \" + str(custom_specificity_score(y_test, y_pred)))\n",
    "print(\"     POSITIVE PREDICTIVE VALUE, (PRECISION): \" + str(custom_ppv_score(y_test, y_pred)))\n",
    "print(\"                  NEGATIVE PREDICTIVE VALUE: \" + str(custom_npv_score(y_test, y_pred)))\n",
    "\n",
    "plot_roc(y_test, y_pred_prob)\n",
    "print(\" AUC: \" + str(roc_auc_score(y_test, y_pred_prob)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resumen de la Regresión Logística: \n",
    "\n",
    "- Primero entrenamos el modelo con el conunto completo de 211 variables\n",
    "- Cuando lo aplicamos a los datos de prueba, AUC=73,2%\n",
    "- Con la selección de variables, AUC aumentó a 85,4%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelo № 2: Árboles de decisión"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos un modelo CART y los llamamos classifier_DT\n",
    "classifier_DT = DecisionTreeClassifier(max_leaf_nodes = 5, random_state=77300) \n",
    "\n",
    "# Entrenamos el modelo classifier_DT con los datos de entrenamiento\n",
    "classifier_DT.fit(X_train, y_train)\n",
    "\n",
    "# Usamos el modelo entrenado para predecir sobre los datos de prueba\n",
    "y_pred_prob = classifier_DT.predict_proba(X_test)[:,1] # probabilidades \n",
    "y_pred = np.where(y_pred_prob > class_threshold, 1, 0) # clasificación\n",
    "\n",
    "print(y_pred_prob[0:5]) # primeras 5 probabilidades \n",
    "print(y_pred[0:5]) # resultados pronosticados \n",
    "print(y_test[0:5]) # resultados reales\n",
    "\n",
    "# WOW -- el mnodelo CART no comete errores sobre los primeros 5 clientes! \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizaicón del árbol resultante\n",
    "\n",
    "from sklearn import tree\n",
    "\n",
    "plt.figure(figsize=(40,20))\n",
    "tree.plot_tree(classifier_DT.fit(X_train, y_train), feature_names = X_train.columns, filled = True, \n",
    "               class_names = ['No-Retenido', 'Retenido'], rounded = True)\n",
    "print('árbol CART con 5 nodos terminales')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Revisemos las métricas del modelo\n",
    "\n",
    "print('Métricas del modelo CART: \\n')\n",
    "\n",
    "cm = np.transpose(confusion_matrix(y_test, y_pred))\n",
    "print(\"Matriz de Confusión: \\n\" + str(cm))\n",
    "\n",
    "print(\"                                   Accuracy: \" + str(custom_accuracy_score(y_test, y_pred))) \n",
    "print(\"                       SENSITIVITY (RECALL): \" + str(custom_sensitivity_score(y_test, y_pred)))\n",
    "print(\"                     SPECIFICITY (FALL-OUT): \" + str(custom_specificity_score(y_test, y_pred)))\n",
    "print(\"     POSITIVE PREDICTIVE VALUE, (PRECISION): \" + str(custom_ppv_score(y_test, y_pred)))\n",
    "print(\"                  NEGATIVE PREDICTIVE VALUE: \" + str(custom_npv_score(y_test, y_pred)))\n",
    "\n",
    "plot_roc(y_test, y_pred_prob)\n",
    "print(\" AUC: \" + str(roc_auc_score(y_test, y_pred_prob)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tuning de hiper-parámetros. Un modelo CART tiene múltiples hiper-parámetros, por ejemplo:\n",
    "# -- máximo número de nodos terminales (hojas) en un árbol, \n",
    "# -- mínimo número de datos en un nodo terminal (hoja)\n",
    "# -- mínimo número de datos para crear una ramificación (split)\n",
    "# y otros\n",
    "\n",
    "DecisionTreeClassifier() # desplegamos cuáles son estos hiper-parámetros y sus valores por defecto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tuning del hiper-parámetro max_leaf_nodes\n",
    "\n",
    "n_max_leaf_nodes = range(5,40) # Entrenamos los modelos con 5, 6, 7, ... 40 hojas\n",
    "\n",
    "# para cada modelo calculamos el AUC para prueba \n",
    "array = []\n",
    "for n in n_max_leaf_nodes:\n",
    "    \n",
    "    classifier_DT = tree.DecisionTreeClassifier(criterion = 'gini', max_leaf_nodes = n)\n",
    "    classifier_DT = classifier_DT.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred_prob = classifier_DT.predict_proba(X_test)[:,1]   \n",
    "    y_pred = np.where(y_pred_prob > class_threshold, 1, 0)\n",
    "\n",
    "    array.append([n,roc_auc_score(y_test, y_pred_prob)])\n",
    "\n",
    "# graficamos los AUCs sobre el dataset de prueba\n",
    "array = pd.DataFrame(array)\n",
    "plt.scatter(array[0],array[1])\n",
    "\n",
    "# ahora, para cada modelo calculamos AUC sobre el dataset de entrenamiento \n",
    "array = []\n",
    "for n in n_max_leaf_nodes:\n",
    "\n",
    "    classifier_DT = tree.DecisionTreeClassifier(criterion = 'gini', max_leaf_nodes = n)\n",
    "    classifier_DT = classifier_DT.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred_prob = classifier_DT.predict_proba(X_train)[:,1] \n",
    "    y_pred = np.where(y_pred_prob > class_threshold, 1, 0)\n",
    "\n",
    "    array.append([n,roc_auc_score(y_train, y_pred_prob)])\n",
    "\n",
    "# graficamos los AUCs sobre el dataset de entrenamiento\n",
    "array = pd.DataFrame(array)\n",
    "plt.scatter(array[0],array[1])\n",
    "\n",
    "# etiquetamos los ejes en el gráfico\n",
    "plt.xlabel('Hiper-parámetro: max_leaf_nodes')\n",
    "plt.ylabel('AUC')\n",
    "\n",
    "# add the legend\n",
    "plt.legend(['Dataset de Prueba','Dataset de Entrenamiento'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Este gráfico ilustra el concepto de \"overfitting\":  \n",
    "\n",
    "- entre más hojas tenga el árbol, más alto es el AUC sobre el dataset de entrenamiento\n",
    "\n",
    "- sin embargo, comenzando a partir de ~8-12 hojas, el AUC de prueba comienza a disminuir, puesto que un modelo que es demasiado compleo \"aprende\" algo que no generaliza a los nuevos datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# entrenemos el modelo con 8 hojas\n",
    "classifier_DT = tree.DecisionTreeClassifier(criterion = 'gini', max_leaf_nodes = 8)\n",
    "classifier_DT = classifier_DT.fit(X_train, y_train)\n",
    "\n",
    "# obtenemos sus predicciones\n",
    "y_pred_prob = classifier_DT.predict_proba(X_test)[:,1]   \n",
    "y_pred = np.where(y_pred_prob > class_threshold, 1, 0)\n",
    "\n",
    "# calculamos e imprimimos el AUC\n",
    "print(\" AUC: \" + str(roc_auc_score(y_test, y_pred_prob)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resumen del CART: \n",
    "\n",
    "- primero, entrenamos el modelo con los hiper-parámetros por defecto\n",
    "- aplicandolo al dataset de prueba, obtuvimos AUC=80.0%\n",
    "- luego, sintonizamos el parámetro max_leaf_nodes y el AUC aumentó a 83.8%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelo № 3: Support Vector Machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "svm_estimators = []\n",
    "svm_estimators.append(('standardize', StandardScaler())) # escalamos los datos\n",
    "svm_estimators.append(('svm', svm.SVC(probability=True))) # definimos SVM con probabilidades \n",
    "     \n",
    "# Definimos el modelo SVM y lo llamamos classifier_SVM\n",
    "Classifier_SVM = Pipeline(svm_estimators, verbose=False)\n",
    "\n",
    "# Entrenamos el modelo classifier_SVM sobre los datos de entrenamiento\n",
    "Classifier_SVM.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usamos el modelo desarrollado, para predecir sobre los datos de prueba \n",
    "y_pred_prob = Classifier_SVM.predict_proba(X_test)[:,1] # probabilidades\n",
    "y_pred = np.where(y_pred_prob > class_threshold, 1, 0) # clasificación\n",
    "\n",
    "# Revisemos las métricas del modelo\n",
    "\n",
    "print('Métricas del modelo de Máquina de Vectores de Soporte: \\n')\n",
    "\n",
    "cm = np.transpose(confusion_matrix(y_test, y_pred))\n",
    "print(\"Matriz de Confusión: \\n\" + str(cm))\n",
    "\n",
    "print(\"                                   Accuracy: \" + str(custom_accuracy_score(y_test, y_pred))) \n",
    "print(\"                       SENSITIVITY (RECALL): \" + str(custom_sensitivity_score(y_test, y_pred)))\n",
    "print(\"                     SPECIFICITY (FALL-OUT): \" + str(custom_specificity_score(y_test, y_pred)))\n",
    "print(\"     POSITIVE PREDICTIVE VALUE, (PRECISION): \" + str(custom_ppv_score(y_test, y_pred)))\n",
    "print(\"                  NEGATIVE PREDICTIVE VALUE: \" + str(custom_npv_score(y_test, y_pred)))\n",
    "\n",
    "plot_roc(y_test, y_pred_prob)\n",
    "print(\" AUC: \" + str(roc_auc_score(y_test, y_pred_prob)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###  Ejecución de SVM con las 20 variables seleccionadas del dataset A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Con el mejor modelo (todas o solo 20 variables del dataset A), desarrollar SVM lineal sintonizando hiper-parámetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Con el mejor modelo (todas o solo 20 variables del dataset A), desarrollar SVM radial sintonizando hiper-parámetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Agregar dataset B al dataset A = dataset completo.  Dividir en entrenamiento y prueba.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Desarrollar modelo de Regresión Logística con todas las variables del dataset completo, y evaluar su desempeño"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Desarrollar modelo de Regresión Logística seleccionando las 20 variables más importantes del dataset completo, \n",
    "### y evaluar su desempeño"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###  Ejecución de SVM con todas las variables del dataset completo, y evaluar desempeño"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###  Ejecución de SVM con las 20 variables seleccionadas del dataset completo, y evaluar desempeño"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Con el mejor modelo (todas o solo 20 variables del dataset A), desarrollar SVM lineal sintonizando hiper-parámetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Con el mejor modelo (todas o solo 20 variables del dataset A), desarrollar SVM radial sintonizando hiper-parámetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resumen del SVM: \n",
    "\n",
    "- entrenamos el modelo con los hiper-parámetros por defecto\n",
    "- aplicando el modelo al dataset de prueba, obtuvimos AUC=85.7%\n",
    "- ahora, sintonizaremos los hiper-parámetros del modelo ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paso 10: Exportar las predicciones del mejor modelo, para uso posterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Si tenemos los datos de predicción (acerca de los nuevos clientes), primero los cargamos y limpliamos para obtener la\n",
    "# matriz exactamente con la misma estructura de X_train\n",
    "# Como no tenemos tales datos, pronosticaremos los datos de prueba y exportaremos las predicciones resultantes\n",
    "\n",
    "X_pred = X_test\n",
    "\n",
    "# Cuál modelo deberíamos tomar? \n",
    "\n",
    "# En estge momento, el mejor modelo es el SVM con hiper-parámetros optimizados.  \n",
    "y_pred_prob = grid_search_SVM.predict_proba(X_pred)[:,1]   \n",
    "\n",
    "# Lets add the ID column to know \"who is who\"\n",
    "Prediction = pd.DataFrame(data={\"ID\":X_pred[\"ID\"],\"Predicted Probability\":y_pred_prob}) \n",
    "\n",
    "# Export the predictions into a CSV file\n",
    "Prediction.to_csv(\"Predicted Retention Probability_testing.csv\",sep = ',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BONUS: Gráfico de Ganancias (\"Lift\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install scikit-plot\n",
    "\n",
    "import scikitplot as skplt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# as an example, here is the gains chart with SVM; note, people often call this \"lift chart\"\n",
    "# note, this is a \"sister-plot\" to the lift chart we saw in class -- take the ratio of the Class 1 and Baseline values to obtain what we saw\n",
    "\n",
    "# La curva de ganancias acumuladas es una curva de evaluación que evalúa el rendimiento del modelo y compara los \n",
    "# resultados con la selección aleatoria. Muestra el porcentaje de targets logrados al considerar un determinado \n",
    "# porcentaje de la población con la mayor probabilidad de ser el target de acuerdo con el modelo.\n",
    "\n",
    "y_pred_proba=Classifier_SVM.predict_proba(X_test)\n",
    "\n",
    "skplt.metrics.plot_cumulative_gain(np.int32(y_test), y_pred_proba)\n",
    "plt.ylabel(\"Cumulative Gain Value\")\n",
    "plt.xlabel(\"Rate of Predictions\")\n",
    "plt.title(\"Cumulative Gain Chart\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}